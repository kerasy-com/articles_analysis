{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from konlpy.tag import Okt\n",
    "#okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Extract File paths and names(only name of that file) from data_korean\n",
    "def search_paths_names(dir):\n",
    "    file_paths = []\n",
    "    file_names = []\n",
    "    files = os.listdir(dir)\n",
    "    for file_ in files:\n",
    "        file_names.append(file_)\n",
    "        filename = os.path.join(dir,file_)\n",
    "        file_paths.append(filename)\n",
    "    return file_paths, file_names\n",
    "\n",
    "file_paths, file_names = search_paths_names('./data_korean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. convert xlsx files into csv files\n",
    "for data in file_names:\n",
    "    start, end = data.split('_')[4], data.split('_')[5].split('(')[0]\n",
    "    data_name, csv = data.split('.')\n",
    "    \n",
    "    articles = pd.read_excel('./data_korean/' + data, encoding = 'cp949', converters={'ART_ID': str})\n",
    "\n",
    "    if not(os.path.isdir('./csv_korean/' + start + '_' + end)):\n",
    "        os.makedirs('./csv_korean/' + start + '_' + end)\n",
    "\n",
    "    try:\n",
    "        articles.to_csv('./csv_korean\\\\' + start + '_' + end + '\\\\' + data_name + \".csv\", sep = ',', encoding = 'cp949', index = False)\n",
    "    except UnicodeEncodeError:\n",
    "        articles.to_csv('./csv_korean\\\\' + start + '_' + end + '\\\\' + data_name + \".csv\", sep = ',', encoding = 'utf8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "file_names = []\n",
    "\n",
    "#3. Copy and Paste folder structure to articles_korean_with_date folder\n",
    "def file_folder_split(dir):\n",
    "    for x in os.listdir(dir):\n",
    "        if os.path.isfile(os.path.join(dir, x)) and \"csv\" in x:\n",
    "            file_paths.append(os.path.join(dir, x))\n",
    "            file_names.append(x)\n",
    "            #print(\"File ==> {0}\".format(x))\n",
    "        if os.path.isdir(os.path.join(dir, x)):\n",
    "            new_dir = os.path.join(dir, x)\n",
    "            new_dir = new_dir.replace(\"./csv_korean\", \"./articles_korean_with_date\")\n",
    "            if not(os.path.isdir(new_dir)):\n",
    "                os.makedirs(new_dir)\n",
    "            #print(\"Folder ==> {0}\".format(x))\n",
    "            file_folder_split(os.path.join(dir, x))\n",
    "\n",
    "file_folder_split('./csv_korean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folders in os.listdir('./articles_korean_with_date'):\n",
    "    for i in range(1, 13):\n",
    "        if not(os.path.isdir(os.path.join('./articles_korean_with_date', folders) + '\\\\' + str(i))):\n",
    "            os.makedirs(os.path.join('./articles_korean_with_date', folders) + '\\\\' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YoungMinAhn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#4. Divide each file by year and months\n",
    "\n",
    "data_index = 0\n",
    "total_index = 0\n",
    "for data in file_paths:\n",
    "    start, end = file_names[data_index].split('_')[4], file_names[data_index].split('_')[5].split('(')[0]\n",
    "\n",
    "    try:\n",
    "        articles = pd.read_csv(data, encoding = 'cp949', converters={'ART_ID': str})\n",
    "    except UnicodeDecodeError:\n",
    "        articles = pd.read_csv(data, encoding = 'utf8', converters={'ART_ID': str})\n",
    "    articles = articles[['ART_ID', 'ART_CONTENT', 'ART_DATE']].dropna(axis = 0)\n",
    "    articles = articles.reset_index()\n",
    "    \n",
    "    articles['DATE'] = pd.to_datetime(articles['ART_DATE'], format = '%Y%m%d')\n",
    "    articles_with_date = articles[['ART_ID', 'ART_CONTENT', 'DATE']]\n",
    "    articles_with_date['MONTH'] = pd.DatetimeIndex(articles_with_date['DATE']).month\n",
    "    \n",
    "    monthly_data = [None for i in range(12)]\n",
    "    for i in range(0, 12):\n",
    "        df = articles_with_date[articles_with_date['MONTH'] == (i+1)]\n",
    "        if(df.empty):\n",
    "            monthly_data[i] = None\n",
    "        else:\n",
    "            monthly_data[i] = df\n",
    "    \n",
    "    for i in range(0, 12):\n",
    "        if monthly_data[i] is None:\n",
    "            total_index += 1\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                monthly_data[i].to_csv('./articles_korean_with_date\\\\'+ start + '_' + end + '\\\\' + str(i+1) + '\\\\'+ str(total_index) + '.csv', index = False, encoding = 'cp949')\n",
    "            except UnicodeEncodeError:\n",
    "                monthly_data[i].to_csv('./articles_korean_with_date\\\\'+ start + '_' + end + '\\\\' + str(i+1) + '\\\\'+ str(total_index) + '.csv', index = False, encoding = 'utf8')\n",
    "            total_index += 1\n",
    "    data_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. For further cleansing and collection.\n",
    "#I will cleanse each data and create 'clenased_articles' column\n",
    "#Then, vectorize the each file according to its month and year\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "file_paths = []\n",
    "file_names = []\n",
    "\n",
    "def file_folder_split(dir):\n",
    "    for x in os.listdir(dir):\n",
    "        if os.path.isfile(os.path.join(dir, x)) and \"csv\" in x:\n",
    "            file_paths.append(os.path.join(dir, x))\n",
    "            file_names.append(x)\n",
    "        if os.path.isdir(os.path.join(dir, x)):\n",
    "            file_folder_split(os.path.join(dir, x))\n",
    "file_folder_split('./articles_korean_with_date')\n",
    "\n",
    "data_index = 0\n",
    "for data in file_paths:\n",
    "    print(data)\n",
    "    try:\n",
    "        articles = pd.read_csv(data, encoding = 'cp949', converters={'ART_ID': str})\n",
    "    except UnicodeDecodeError:\n",
    "        articles = pd.read_csv(data, encoding = 'utf8', converters={'ART_ID': str})\n",
    "    articles = articles[['ART_ID', 'ART_CONTENT', 'MONTH']].dropna(axis = 0)\n",
    "    articles = articles.reset_index(drop = True)\n",
    "\n",
    "    id = articles[['ART_ID', 'MONTH']]\n",
    "    text = articles['ART_CONTENT'].values.tolist()\n",
    "\n",
    "    # 1. For each sentence, imported the tagger(in this case, Twitter) and preprocessed.\n",
    "    sentences_tag = []\n",
    "    for sentence in text:\n",
    "        morph = okt.pos(sentence, stem = True)\n",
    "        sentences_tag.append(morph)\n",
    "\n",
    "    # 2. Put in words that are noun/verb/adjective\n",
    "    noun_adj_list = []\n",
    "    for sentence1 in sentences_tag:\n",
    "        words = \"\"\n",
    "        for word, tag in sentence1:\n",
    "            if tag in ['Noun','Adjective', 'Verb']:\n",
    "                words += word + \", \"\n",
    "        noun_adj_list.append(words)\n",
    "\n",
    "    noun_adj_df = pd.DataFrame(noun_adj_list, columns = ['ART_CONTENT_CLEAN'])\n",
    "    # 3. Concated the ARTICLES_ID and saved the file.\n",
    "    count_vec_df_concat = pd.concat([id, noun_adj_df], axis = 1)\n",
    "\n",
    "    try:\n",
    "        count_vec_df_concat.to_csv(data, index = False, encoding = 'cp949')\n",
    "    except UnicodeEncodeError:\n",
    "        count_vec_df_concat.to_csv(data, index = False, encoding = 'utf8')\n",
    "    data_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "temp2 = []\n",
    "def file_folder_split_temp(dir):\n",
    "    data = []\n",
    "    for x in os.listdir(dir):\n",
    "        if os.path.isfile(os.path.join(dir, x)) and (\"csv\" in x):\n",
    "            data.append(os.path.join(dir, x))\n",
    "            temp2.append(x)\n",
    "            #print(\"File ==> {0}\".format(x))\n",
    "        if os.path.isdir(os.path.join(dir, x)):\n",
    "            #print(\"Folder ==> {0}\".format(x))\n",
    "            file_folder_split_temp(os.path.join(dir, x))\n",
    "    \n",
    "    if data:\n",
    "        temp.append(data)\n",
    "file_folder_split_temp('articles_korean_with_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "for data_list in temp:\n",
    "    partial_articles = []\n",
    "    year = data_list[0].split('\\\\')[1]\n",
    "    print(year)\n",
    "    for data in data_list:\n",
    "        try:\n",
    "            articles = pd.read_csv(data, encoding = 'cp949', converters={'ART_ID': str})\n",
    "        except UnicodeDecodeError:\n",
    "            articles = pd.read_csv(data, encoding = 'utf8', converters={'ART_ID': str})\n",
    "        \n",
    "        partial_articles.append(articles)\n",
    "        data_index += 1\n",
    "    \n",
    "    all_articles = partial_articles[0]\n",
    "    for i in range(1, len(partial_articles)):\n",
    "        all_articles = pd.concat([all_articles, partial_articles[i]])\n",
    "    \n",
    "    try:\n",
    "        all_articles.to_csv('all_articles_korean_' + str(year) + '.csv', index = False, encoding = 'cp949')\n",
    "    except UnicodeEncodeError:\n",
    "        all_articles.to_csv('all_articles_korean_' + str(year) + '.csv', index = False, encoding = 'utf8')\n",
    "\n",
    "    all_id = all_articles['ART_ID']\n",
    "    month = int(all_articles['MONTH'].mean())\n",
    "\n",
    "    vec = CountVectorizer()\n",
    "    text_vec = vec.fit_transform(all_articles.ART_CONTENT_CLEAN.values.astype(str))\n",
    "    count_vec_df = pd.DataFrame(text_vec.todense(), columns = vec.get_feature_names())\n",
    "    all_id = all_id.reset_index(drop=True)\n",
    "    yearly_vec = pd.concat([all_id, count_vec_df], axis = 1)\n",
    "    \n",
    "    try:\n",
    "        yearly_vec.to_csv('all_articles_korean_vec_' + str(year) + \"_\" + str(month) + '.csv', index = False, encoding = 'cp949')\n",
    "    except UnicodeEncodeError:\n",
    "        yearly_vec.to_csv('all_articles_korean_vec_' + str(year) + \"_\" + str(month) + '.csv', index = False, encoding = 'utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
